{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JaQtae\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# Importing data from the r/politics\n",
    "from psaw import PushshiftAPI\n",
    "from datetime import datetime, timezone, date\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from nltk.draw.dispersion import dispersion_plot\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "from PIL import Image\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "def setup_mpl():\n",
    "    #mpl.rcParams['font.family'] = 'Helvetica Neue'\n",
    "    mpl.rcParams['font.size'] = '14'\n",
    "    mpl.rcParams['lines.linewidth'] = 3\n",
    "setup_mpl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud\n",
    "Loading in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>score</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_author</th>\n",
       "      <th>tokens</th>\n",
       "      <th>politician</th>\n",
       "      <th>children_comments</th>\n",
       "      <th>mentions_Trump</th>\n",
       "      <th>mentions_Biden</th>\n",
       "      <th>compound_sentiment_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:26:26</th>\n",
       "      <td>gay6kwb</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>t1_gay5x5w</td>\n",
       "      <td>Division is his weapon</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>['division', 'weapon']</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:20:27</th>\n",
       "      <td>gay5x5w</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>t1_gay5pjk</td>\n",
       "      <td>dunno, everything trump does seems to generate...</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>['dunno', 'everything', 'trump', 'seems', 'gen...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['gay6kwb']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:18:33</th>\n",
       "      <td>gay5pjk</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>t1_gay5doq</td>\n",
       "      <td>Not sure it generates many more votes. I work ...</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>['not', 'sure', 'generates', 'many', 'votes', ...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['gay5x5w']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:15:36</th>\n",
       "      <td>gay5doq</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>t1_gay3si8</td>\n",
       "      <td>yeah, but for every vote this generates, would...</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>['yeah', 'every', 'vote', 'generates', 'would'...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['gay5pjk']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:04:18</th>\n",
       "      <td>gay43vt</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>Tesides</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>I really do believe this idiot is trying to lo...</td>\n",
       "      <td>geoxol</td>\n",
       "      <td>['i', 'really', 'believe', 'idiot', 'trying', ...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id    link_id  score        author   parent_id  \\\n",
       "dates                                                                      \n",
       "2020-11-02 23:26:26  gay6kwb  t3_jmybs3      1  J_Class_Ford  t1_gay5x5w   \n",
       "2020-11-02 23:20:27  gay5x5w  t3_jmybs3      1     cyanydeez  t1_gay5pjk   \n",
       "2020-11-02 23:18:33  gay5pjk  t3_jmybs3      1  J_Class_Ford  t1_gay5doq   \n",
       "2020-11-02 23:15:36  gay5doq  t3_jmybs3      1     cyanydeez  t1_gay3si8   \n",
       "2020-11-02 23:04:18  gay43vt  t3_jmybs3      1       Tesides   t3_jmybs3   \n",
       "\n",
       "                                                                  body  \\\n",
       "dates                                                                    \n",
       "2020-11-02 23:26:26                             Division is his weapon   \n",
       "2020-11-02 23:20:27  dunno, everything trump does seems to generate...   \n",
       "2020-11-02 23:18:33  Not sure it generates many more votes. I work ...   \n",
       "2020-11-02 23:15:36  yeah, but for every vote this generates, would...   \n",
       "2020-11-02 23:04:18  I really do believe this idiot is trying to lo...   \n",
       "\n",
       "                    parent_author  \\\n",
       "dates                               \n",
       "2020-11-02 23:26:26     cyanydeez   \n",
       "2020-11-02 23:20:27  J_Class_Ford   \n",
       "2020-11-02 23:18:33     cyanydeez   \n",
       "2020-11-02 23:15:36  J_Class_Ford   \n",
       "2020-11-02 23:04:18        geoxol   \n",
       "\n",
       "                                                                tokens  \\\n",
       "dates                                                                    \n",
       "2020-11-02 23:26:26                             ['division', 'weapon']   \n",
       "2020-11-02 23:20:27  ['dunno', 'everything', 'trump', 'seems', 'gen...   \n",
       "2020-11-02 23:18:33  ['not', 'sure', 'generates', 'many', 'votes', ...   \n",
       "2020-11-02 23:15:36  ['yeah', 'every', 'vote', 'generates', 'would'...   \n",
       "2020-11-02 23:04:18  ['i', 'really', 'believe', 'idiot', 'trying', ...   \n",
       "\n",
       "                    politician children_comments mentions_Trump  \\\n",
       "dates                                                             \n",
       "2020-11-02 23:26:26      Trump                []            NaN   \n",
       "2020-11-02 23:20:27      Trump       ['gay6kwb']           True   \n",
       "2020-11-02 23:18:33      Trump       ['gay5x5w']           True   \n",
       "2020-11-02 23:15:36      Trump       ['gay5pjk']            NaN   \n",
       "2020-11-02 23:04:18      Trump                []            NaN   \n",
       "\n",
       "                    mentions_Biden  compound_sentiment_score  \n",
       "dates                                                         \n",
       "2020-11-02 23:26:26            NaN                   -0.2960  \n",
       "2020-11-02 23:20:27            NaN                    0.0000  \n",
       "2020-11-02 23:18:33            NaN                   -0.0160  \n",
       "2020-11-02 23:15:36            NaN                    0.5450  \n",
       "2020-11-02 23:04:18            NaN                   -0.7632  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_storage = local_storage = r\"C:\\Users\\JaQtae\\Desktop\\SocInfo2022\\Data\\FINAL_COMMENTS.csv\"\n",
    "com_data = pd.read_csv(local_storage,index_col=0,parse_dates=[0])\n",
    "com_data.index = com_data.index.rename('dates')\n",
    "# The collected comments data set still contained some data from 2020-09-30. This is removed. \n",
    "\n",
    "# Make sure all text bodies are of type string. \n",
    "com_data[\"body\"] = com_data[\"body\"].apply(str)\n",
    "com_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the data by the associated politician and make it into a single corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politician\n",
       "Biden    [I'm crazy. I'm not evil. I'm just like many o...\n",
       "Trump    [Division is his weapon, dunno, everything tru...\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_bodies = com_data.groupby([\"politician\"]).apply(lambda x: x[\"body\"].unique())\n",
    "author_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_corpus = \" \".join(author_bodies[\"Trump\"])\n",
    "biden_corpus = \" \".join(author_bodies[\"Biden\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Includes removal of stopwords, URLS and normalizes the text by lowercasing all characters.\n",
    "Note: We could remove numbers?\n",
    "\n",
    "We apply this to the corpus', and remove webscraping occurences of &gt and &amp for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We solve this by defining the clean_tokens function below.\n",
    "# Define stop words to also include punctuation\n",
    "import nltk\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Function to tokenize and clean the text of each submission\n",
    "def clean_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # In the list comprehension below, we exclude URL's, stopwords and numbers as well as setting all characters to lowercase\n",
    "    \n",
    "    clean_tokens = [re.sub(r'http\\S+', '', str(i)).lower() for i in tokens if str(i).isalpha()]\n",
    "    clean_tokens = [t for t in clean_tokens if t not in stop]\n",
    "    return clean_tokens\n",
    "\n",
    "TTC = clean_tokens(trump_corpus)\n",
    "TBC = clean_tokens(biden_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTC_j = \" \".join(TTC)\n",
    "TBC_j = \" \".join(TBC)\n",
    "# Hader regex\n",
    "TTC_j = TTC_j.replace(' gt', '')\n",
    "TTC_j = TTC_j.replace(' amp', '')\n",
    "TBC_j = TBC_j.replace(' gt', '')\n",
    "TBC_j = TBC_j.replace(' amp', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 91672/91672 [00:28<00:00, 3269.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define stop words to also include punctuation\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "# Function to tokenize and clean the text of each submission\n",
    "def clean_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    clean_tokens = [re.sub(r'http\\S+', '', str(i)).lower() for i in tokens if str(i).isalpha()]\n",
    "    clean_tokens = [t for t in clean_tokens if t not in stop]\n",
    "    return clean_tokens\n",
    "\n",
    "com_data[\"tokens\"] = com_data.progress_apply(lambda x: clean_tokens(x[\"body\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>score</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_author</th>\n",
       "      <th>tokens</th>\n",
       "      <th>politician</th>\n",
       "      <th>children_comments</th>\n",
       "      <th>mentions_Trump</th>\n",
       "      <th>mentions_Biden</th>\n",
       "      <th>compound_sentiment_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:26:26</th>\n",
       "      <td>gay6kwb</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>t1_gay5x5w</td>\n",
       "      <td>Division is his weapon</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>[division, weapon]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:20:27</th>\n",
       "      <td>gay5x5w</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>t1_gay5pjk</td>\n",
       "      <td>dunno, everything trump does seems to generate...</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>[dunno, everything, trump, seems, generate, re...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['gay6kwb']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:18:33</th>\n",
       "      <td>gay5pjk</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>t1_gay5doq</td>\n",
       "      <td>Not sure it generates many more votes. I work ...</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>[sure, generates, many, votes, work, assumptio...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['gay5x5w']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:15:36</th>\n",
       "      <td>gay5doq</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>cyanydeez</td>\n",
       "      <td>t1_gay3si8</td>\n",
       "      <td>yeah, but for every vote this generates, would...</td>\n",
       "      <td>J_Class_Ford</td>\n",
       "      <td>[yeah, every, vote, generates, would, think, c...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['gay5pjk']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 23:04:18</th>\n",
       "      <td>gay43vt</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>1</td>\n",
       "      <td>Tesides</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>I really do believe this idiot is trying to lo...</td>\n",
       "      <td>geoxol</td>\n",
       "      <td>[really, believe, idiot, trying, lose, purpose]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.7632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 01:25:30</th>\n",
       "      <td>g78ozrz</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>1</td>\n",
       "      <td>5DollarHitJob</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>Can they just appeal this?</td>\n",
       "      <td>Plymouth03</td>\n",
       "      <td>[appeal]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 01:24:25</th>\n",
       "      <td>g78ovcd</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>1</td>\n",
       "      <td>AmishTechno</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>And, nothing will happen.</td>\n",
       "      <td>Plymouth03</td>\n",
       "      <td>[nothing, happen]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:29:42</th>\n",
       "      <td>g78j6ey</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>1</td>\n",
       "      <td>ryhaltswhiskey</td>\n",
       "      <td>t1_g78iult</td>\n",
       "      <td>Ah I did not know that\\r\\n\\r\\nMan I hope Barr ...</td>\n",
       "      <td>memepolizia</td>\n",
       "      <td>[ah, know, man, hope, barr, ends, jail]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:07:18</th>\n",
       "      <td>g78gwrv</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>1</td>\n",
       "      <td>GrumpyOlBastard</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>So I’m wondering what’s going to happen when n...</td>\n",
       "      <td>Plymouth03</td>\n",
       "      <td>[wondering, going, happen, nothing, happens]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:00:36</th>\n",
       "      <td>g78g8dl</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>1</td>\n",
       "      <td>maybe-fuck-you</td>\n",
       "      <td>t3_j2vwwt</td>\n",
       "      <td>My report I paid for.</td>\n",
       "      <td>Plymouth03</td>\n",
       "      <td>[report, paid]</td>\n",
       "      <td>Trump</td>\n",
       "      <td>['g79ee6w']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91672 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id    link_id  score           author   parent_id  \\\n",
       "dates                                                                         \n",
       "2020-11-02 23:26:26  gay6kwb  t3_jmybs3      1     J_Class_Ford  t1_gay5x5w   \n",
       "2020-11-02 23:20:27  gay5x5w  t3_jmybs3      1        cyanydeez  t1_gay5pjk   \n",
       "2020-11-02 23:18:33  gay5pjk  t3_jmybs3      1     J_Class_Ford  t1_gay5doq   \n",
       "2020-11-02 23:15:36  gay5doq  t3_jmybs3      1        cyanydeez  t1_gay3si8   \n",
       "2020-11-02 23:04:18  gay43vt  t3_jmybs3      1          Tesides   t3_jmybs3   \n",
       "...                      ...        ...    ...              ...         ...   \n",
       "2020-10-01 01:25:30  g78ozrz  t3_j2vwwt      1    5DollarHitJob   t3_j2vwwt   \n",
       "2020-10-01 01:24:25  g78ovcd  t3_j2vwwt      1      AmishTechno   t3_j2vwwt   \n",
       "2020-10-01 00:29:42  g78j6ey  t3_j2vwwt      1   ryhaltswhiskey  t1_g78iult   \n",
       "2020-10-01 00:07:18  g78gwrv  t3_j2vwwt      1  GrumpyOlBastard   t3_j2vwwt   \n",
       "2020-10-01 00:00:36  g78g8dl  t3_j2vwwt      1   maybe-fuck-you   t3_j2vwwt   \n",
       "\n",
       "                                                                  body  \\\n",
       "dates                                                                    \n",
       "2020-11-02 23:26:26                             Division is his weapon   \n",
       "2020-11-02 23:20:27  dunno, everything trump does seems to generate...   \n",
       "2020-11-02 23:18:33  Not sure it generates many more votes. I work ...   \n",
       "2020-11-02 23:15:36  yeah, but for every vote this generates, would...   \n",
       "2020-11-02 23:04:18  I really do believe this idiot is trying to lo...   \n",
       "...                                                                ...   \n",
       "2020-10-01 01:25:30                         Can they just appeal this?   \n",
       "2020-10-01 01:24:25                          And, nothing will happen.   \n",
       "2020-10-01 00:29:42  Ah I did not know that\\r\\n\\r\\nMan I hope Barr ...   \n",
       "2020-10-01 00:07:18  So I’m wondering what’s going to happen when n...   \n",
       "2020-10-01 00:00:36                              My report I paid for.   \n",
       "\n",
       "                    parent_author  \\\n",
       "dates                               \n",
       "2020-11-02 23:26:26     cyanydeez   \n",
       "2020-11-02 23:20:27  J_Class_Ford   \n",
       "2020-11-02 23:18:33     cyanydeez   \n",
       "2020-11-02 23:15:36  J_Class_Ford   \n",
       "2020-11-02 23:04:18        geoxol   \n",
       "...                           ...   \n",
       "2020-10-01 01:25:30    Plymouth03   \n",
       "2020-10-01 01:24:25    Plymouth03   \n",
       "2020-10-01 00:29:42   memepolizia   \n",
       "2020-10-01 00:07:18    Plymouth03   \n",
       "2020-10-01 00:00:36    Plymouth03   \n",
       "\n",
       "                                                                tokens  \\\n",
       "dates                                                                    \n",
       "2020-11-02 23:26:26                                 [division, weapon]   \n",
       "2020-11-02 23:20:27  [dunno, everything, trump, seems, generate, re...   \n",
       "2020-11-02 23:18:33  [sure, generates, many, votes, work, assumptio...   \n",
       "2020-11-02 23:15:36  [yeah, every, vote, generates, would, think, c...   \n",
       "2020-11-02 23:04:18    [really, believe, idiot, trying, lose, purpose]   \n",
       "...                                                                ...   \n",
       "2020-10-01 01:25:30                                           [appeal]   \n",
       "2020-10-01 01:24:25                                  [nothing, happen]   \n",
       "2020-10-01 00:29:42            [ah, know, man, hope, barr, ends, jail]   \n",
       "2020-10-01 00:07:18       [wondering, going, happen, nothing, happens]   \n",
       "2020-10-01 00:00:36                                     [report, paid]   \n",
       "\n",
       "                    politician children_comments mentions_Trump  \\\n",
       "dates                                                             \n",
       "2020-11-02 23:26:26      Trump                []            NaN   \n",
       "2020-11-02 23:20:27      Trump       ['gay6kwb']           True   \n",
       "2020-11-02 23:18:33      Trump       ['gay5x5w']           True   \n",
       "2020-11-02 23:15:36      Trump       ['gay5pjk']            NaN   \n",
       "2020-11-02 23:04:18      Trump                []            NaN   \n",
       "...                        ...               ...            ...   \n",
       "2020-10-01 01:25:30      Trump                []            NaN   \n",
       "2020-10-01 01:24:25      Trump                []            NaN   \n",
       "2020-10-01 00:29:42      Trump                []            NaN   \n",
       "2020-10-01 00:07:18      Trump                []            NaN   \n",
       "2020-10-01 00:00:36      Trump       ['g79ee6w']            NaN   \n",
       "\n",
       "                    mentions_Biden  compound_sentiment_score  \n",
       "dates                                                         \n",
       "2020-11-02 23:26:26            NaN                   -0.2960  \n",
       "2020-11-02 23:20:27            NaN                    0.0000  \n",
       "2020-11-02 23:18:33            NaN                   -0.0160  \n",
       "2020-11-02 23:15:36            NaN                    0.5450  \n",
       "2020-11-02 23:04:18            NaN                   -0.7632  \n",
       "...                            ...                       ...  \n",
       "2020-10-01 01:25:30            NaN                    0.0000  \n",
       "2020-10-01 01:24:25            NaN                    0.0000  \n",
       "2020-10-01 00:29:42            NaN                    0.4404  \n",
       "2020-10-01 00:07:18            NaN                    0.0000  \n",
       "2020-10-01 00:00:36            NaN                    0.0000  \n",
       "\n",
       "[91672 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_data.to_csv(\"FINAL_COMMENTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TTC_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biden_mask = np.array(Image.open(\"Illustrations/Biden_silhouette_background.png\"))\n",
    "wcB = WordCloud(background_color=\"white\", \n",
    "                      mask=Biden_mask,\n",
    "                      contour_width=3, \n",
    "                      repeat=True,\n",
    "                      min_font_size=3,\n",
    "                      contour_color='darkgreen').generate(TBC_j)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(wcB, interpolation='bilinear')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "#wcB.to_file('WordCloud_Biden.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trump_mask = np.array(Image.open(\"Illustrations/Trump_silhouette_background.png\"))\n",
    "wcT = WordCloud(background_color=\"white\", \n",
    "                      mask=Trump_mask,\n",
    "                      contour_width=3, \n",
    "                      repeat=True,\n",
    "                      min_font_size=3,\n",
    "                      contour_color='darkgreen').generate(TTC_j)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(wcT, interpolation='bilinear')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "#wcT.to_file('WordCloud_Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "wcT = WordCloud(width=1400, height=800).generate(TTC_j)\n",
    "plt.figure(figsize=(28,16))\n",
    "plt.imshow(wcT, interpolation='bilinear')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "wcT.to_file('WordCloud_Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "wcB = WordCloud(width=1400, height=800).generate(TBC_j)\n",
    "plt.figure(figsize=(28,16))\n",
    "plt.imshow(wcB, interpolation='bilinear')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "wcB.to_file('WordCloud_Biden.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of dictionary based methods\n",
    "In order for it to work, the rule-of-thumb is to have at least 10.000 tokens per document. We group them daily over the period and plot it with the desired cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_data['cleaned_tokens'] = com_data['body'].progress_apply(lambda x: clean_tokens(x))\n",
    "com_data['date'] = com_data.index\n",
    "com_data['test'] = com_data['date'].apply(lambda x: datetime.strptime(str(x), \"%Y-%m-%d %H:%M:%S\").strftime('%Y-%m-%d'))\n",
    "documents_per_day = com_data.groupby(\"test\").cleaned_tokens.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(documents_per_day))):\n",
    "    #Remove gt's and amp's\n",
    "    c1 = documents_per_day[i].count('gt')\n",
    "    c2 = documents_per_day[i].count('amp')\n",
    "    k = max(c1, c2)\n",
    "    for j in range(0,k):\n",
    "        try:\n",
    "            documents_per_day[i].remove('gt')\n",
    "            documents_per_day[i].remove('amp')\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    #Remove nothings ''\n",
    "    c3 = documents_per_day[i].count('')\n",
    "    for j in range(0,c3):\n",
    "        try:\n",
    "            documents_per_day[i].remove('')\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-scaled for better viewablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "fig, ax = plt.subplots(figsize=(6,3), dpi=400)\n",
    "#plt.xticks(rotation='vertical')\n",
    "ax.bar(documents_per_day.index, [len(doc) for doc in documents_per_day], color= \"b\")\n",
    "ax.set_title(\"Daily length of documents\")\n",
    "ax.axhline(10000, color=\"r\", linestyle=\"dashed\", label=\"Cutoff\")\n",
    "ax.set_ylabel(\"Number of tokens (log)\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(7))\n",
    "plt.xticks(rotation = 25)\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We disregard the last 3 days in the analysis going forward, partially due to them not living up to the criteria for the methods used, but also due to the difference in comments each day for each candidate, ie. Trump has comments on 2020-11-04, while Biden does not, whereas the reverse is true for 2020-11-04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happiness scores using the Hedonometer lexicon.\n",
    "We load in the Hedonometer data regarding happiness scores for words and apply it in the analysis of the daily documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT = pd.read_csv(r\"C:\\Users\\JaQtae\\Desktop\\SocInfo2022\\Hedonometer.csv\", index_col=\"Word\")\n",
    "setup_mpl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hscore(tokens, p=False):\n",
    "    #Iterative counter\n",
    "    # If we want print p = True.\n",
    "    score = 0\n",
    "    freq_dict = dict(nltk.FreqDist(tokens))\n",
    "    freq_norm = 0\n",
    "    for word in tokens:\n",
    "        # Count for word in dict\n",
    "        try:\n",
    "            score += labMT.loc[[word]][\"Happiness Score\"].values[0] * freq_dict[word]\n",
    "            freq_norm += freq_dict[word]\n",
    "        except:\n",
    "            #Doesnt exist in labMT\n",
    "            None\n",
    "    hscore = score / (freq_norm+1e-06) #NaN for 0-division\n",
    "    if p==True:\n",
    "        return print(\"Happiness score: {}\".format(hscore))\n",
    "    else:\n",
    "        return hscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = com_data.groupby(['test', 'politician']).cleaned_tokens.sum()\n",
    "for i in tqdm(range(len(temp))):\n",
    "    #Removal of webscrape nonsense\n",
    "    c1 = temp[i].count('gt')\n",
    "    c2 = temp[i].count('amp')\n",
    "    k = max(c1, c2)\n",
    "    for j in range(0,k):\n",
    "        try:\n",
    "            temp[i].remove('gt')\n",
    "            temp[i].remove('amp')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    c3 = temp[i].count('')\n",
    "    for j in range(0,c3):\n",
    "        try:\n",
    "            temp[i].remove('')\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_daily_hscore = temp.progress_apply(lambda x: hscore(x))\n",
    "P_daily_hscore.index = temp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to csv for ease of plotting\n",
    "#P_daily_hscore.to_csv(\"P_daily_hscore.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we redefine the dataset to another format that is more maleable, ie. pandas dataframes with the indexing wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_hscores = []\n",
    "T_hscores = []\n",
    "for q in range(len(P_daily_hscore)):\n",
    "    if P_daily_hscore.index[q].count('Biden') == 1:\n",
    "        B_hscores.append(P_daily_hscore[q])\n",
    "    elif P_daily_hscore.index[q].count('Trump') == 1:\n",
    "        T_hscores.append(P_daily_hscore[q])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "len(B_hscores) == len(T_hscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframes are easier to manage in the form from Week7\n",
    "df_B = pd.DataFrame()\n",
    "df_T = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B['B_hscore'] = B_hscores\n",
    "df_T['T_hscore'] = T_hscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcher = []\n",
    "tester = []\n",
    "for k in range(len(P_daily_hscore)):\n",
    "    if P_daily_hscore.index[k].count('Biden') == 1:\n",
    "        catcher.append(P_daily_hscore.index[k][0])\n",
    "    elif P_daily_hscore.index[k].count('Trump') == 1:\n",
    "        tester.append(P_daily_hscore.index[k][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The values are dropped in correspondence with the previous comments regarding cut-off of the tail end of the dictionary based methodology criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del catcher[-2:]\n",
    "del tester[-2:]\n",
    "df_B.drop(df_B.tail(2).index,inplace=True)\n",
    "df_T.drop(df_T.tail(2).index,inplace=True)\n",
    "df_B.index = catcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4), dpi=400)\n",
    "ax.plot(df_B.index, df_B, ls = \"--\", alpha = 0.5, label='Biden')\n",
    "ax.plot(df_T.index, df_T, ls = \"--\", alpha = 0.5, label='Trump')\n",
    "plt.axvline(x=df_B.index[-1], color='green', alpha = 0.4, label='Election Day')\n",
    "ax.set_title(\"Avg. daily happiness ratings \\n r/politics submission comments\")\n",
    "ax.set_ylabel(\"Avg. happiness\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(8))\n",
    "plt.xticks(rotation = 25)\n",
    "plt.ylim(5.1,7.1)\n",
    "plt.grid(alpha=0.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Shifterator plots\n",
    "We chose the election day as the day to remark upon, and compare it to the whole previous part of the dataset to investigate changes of sentiment over time. This amounts to a total of 33 days contained in the reference dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_day = P_daily_hscore.index[-5][0]\n",
    "print('Election day is: {}'.format(election_day))\n",
    "d_T = datetime.strptime(P_daily_hscore.index[-5][0], '%Y-%m-%d')\n",
    "d_m = (d_T- dt.timedelta(days=33)).strftime('%Y-%m-%d') #string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = documents_per_day.loc[election_day]\n",
    "l_ref = np.concatenate(documents_per_day[(documents_per_day.index < election_day) & \\\n",
    "                                (documents_per_day.index > d_m)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_l = dict([(item[0], item[1]/len(l)) for item in Counter(l).items()])\n",
    "p_lref = dict([(item[0], item[1]/len(l)) for item in Counter(l_ref).items()])\n",
    "sorted(p_lref.items(), key = lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(p_l.items(), key = lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the main topic surrounds trump and people, however on the day of election voting seems to be more significant, understandably so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(token, diff(p_l, p_lref))]\n",
    "# Do it for every word in the union of sets of words for corpora\n",
    "d_p = dict([(token, p_l.get(token, 0) -  p_lref.get(token, 0)) \\\n",
    "               for token in set(p_l.keys()).union(set(p_lref.keys()))])\n",
    "sorted(d_p.items(), key = lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ^This doesn't tell us much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT_dict = pd.Series(labMT[\"Happiness Score\"].values, index=labMT.index).to_dict()\n",
    "prep_hscore =  dict([(token, labMT_dict.get(token, np.nan)-5) for token in set(p_l.keys()).union(set(p_lref.keys()))])\n",
    "\n",
    "d_phi = dict([(token, prep_hscore[token]*d_p[token]) for token in set(p_l.keys()).union(set(p_lref.keys()))\\\n",
    "                    if not np.isnan(prep_hscore[token])]) #Do it for everything that isn't NaN\n",
    "# Absolute value sorting top 10\n",
    "sorted(d_phi.items(), key = lambda x: np.abs(x[1]), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\delta\\phi$ value identifies the extent a given set of words contribute to the difference in scores and how they do it. Either a word is used more often in one text over the other and its influence, positive or negative, is weighted in the relative frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shifterator as sft\n",
    "senti_shift = sft.WeightedAvgShift(type2freq_1 = p_lref,\n",
    "                                     type2freq_2 = p_l,\n",
    "                                     type2score_1 = labMT_dict,\n",
    "                                     reference_value = 5)\n",
    "senti_shift.get_shift_graph(detailed = True,\n",
    "                            system_names = ['ref', 'd'],\n",
    "                           top_n=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gone down on by 0.04 in comparison (neglible imo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
