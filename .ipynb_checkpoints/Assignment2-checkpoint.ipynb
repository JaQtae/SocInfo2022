{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/lalessan/comsocsci2022/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday, April 5th at 23:55. Hand in your Jupyter notebook file (with extension `.ipynb`) via DTU Learn _(Course Content, Assignemnts, Assignment 2)_\n",
    "\n",
    "\n",
    "Remember to include in the first cell of your notebook:\n",
    "* the link to your group's Git repository\n",
    "* group members' contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you need the following data: \n",
    "* The r/wallstreetbets submissions (either the one provided by me [here](https://github.com/lalessan/comsocsci2021/blob/master/data/wallstreet_subs.csv.gz) or the one you downloaded in Week 6).\n",
    "* The list of 15 stocks you identified in Week 6, Exercise 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "    \n",
    "# Downloading the r/wallstreetbets submissions from the GitHub account\n",
    "url = \"https://raw.githubusercontent.com/JaQtae/SocInfo2022/main/wallstreet_subs1.csv\"\n",
    "download = requests.get(url).content\n",
    "\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column containing both the title and the textual content of each submission - the so called __text__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"selftext\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recreate the list of 15 stocks identified in week 6. <br>\n",
    "We start by finding all ticker symbols contained in the text of each submission.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_tickers(text):\n",
    "    tickers = re.findall(r\"[$][a-zA-Z]+\", text)\n",
    "    tickers_uppercase = [t.upper()[1:] for t in tickers] # indexing to remove '$'\n",
    "    tickers = \" \".join(tickers_uppercase)\n",
    "    return tickers\n",
    "                         \n",
    "df[\"tickers\"] = df.apply(lambda x: find_tickers(x[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the list of the 15 most frequently occuring ticker symbols using a frequency distribution from the NLTK library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all tickers to a string.\n",
    "all_tickers = df[\"tickers\"].str.cat(sep=' ')\n",
    "# Tokenize to use freqdist\n",
    "tokens = nltk.word_tokenize(all_tickers)\n",
    "\n",
    "fd = nltk.FreqDist(sorted(tokens, reverse=True))\n",
    "top_15 = fd.most_common(15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPY',\n",
       " 'TSLA',\n",
       " 'SPCE',\n",
       " 'PLTR',\n",
       " 'MSFT',\n",
       " 'ROPE',\n",
       " 'AAPL',\n",
       " 'AMZN',\n",
       " 'NIO',\n",
       " 'ZM',\n",
       " 'AMD',\n",
       " 'BABA',\n",
       " 'GME',\n",
       " 'DIS',\n",
       " 'BA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_15_tickers = [ticker[0] for ticker in top_15]\n",
    "top_15_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise_\n",
    "\n",
    "\n",
    "> 1. Tokenize the __text__ of each submission. Create a column __tokens__ in your dataframe containing the tokens. Remember to follow the instructions in Week 6, Exercise 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve this by defining the clean_tokens function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Define stop words to also include punctuation\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Function to tokenize and clean the text of each submission\n",
    "def clean_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # In the list comprehension below, we exclude URL's, stopwords and numbers as well as setting all characters to lowercase\n",
    "    clean_tokens = [re.sub(r'http', '', i).lower() for i in tokens if (i not in stop and str(i).isalpha())]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df.apply(lambda x: clean_tokens(x[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the tokens column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [what, fed, actually, buying, okay, i, may, ac...\n",
       "1    [i, learn, puts, i, lazy, beginning, virus, sh...\n",
       "2    [hot, take, literally, everyone, free, time, h...\n",
       "3    [fuck, gordon, gordon, i, believed, i, ca, eve...\n",
       "4    [can, find, picture, someone, uploaded, ohoto,...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Find submissions discussing at least one of the top 15 stocks you identified above (follow the instructions in Week 6, Exercise 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column containing a list of the stock tickers\n",
    "df[\"tickers_list\"] = df.apply(lambda x: x[\"tickers\"].split(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions finds the intersection between the listed tickers and the top 15 tickers\n",
    "def find_top_tickers(list_of_tokens):\n",
    "    return [t for t in list_of_tokens if t in top_15_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new column \"stock\" containing the output of the find_top_tickers function applied to the \"tickers_list\" column\n",
    "# Note that the \"tickers_list\" column already contains the ticker symbols included in the text of the submission. \n",
    "df[\"stock\"] = df.apply(lambda x: find_top_tickers(x[\"tickers_list\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we handle cases where one post discusses more than one stock by applying the explode function to the \"stock\" column. \n",
    "df = df.explode(\"stock\")\n",
    "df[\"stock\"] = df[\"stock\"].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex df to remove duplicate indices. \n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stock_subs_idx = df.index[df['stock'] != \"Other\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now investigate some submissions discussing at least one of the 15 most popular stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMISSION DISCUSSING 'ROPE':\n",
      "Don’t sell your OTM puts tomorrow morning I hope this doesn’t age well but tomorrow’s gonna be a blood bath for bear gang.  Between the Delta, Vega, and Theta driven declines you’re probably going to be thinking about $ROPE all day.  \n",
      "\n",
      "Much like dead cat bounces, Tuesday through Thursday should show us at least one day of market correction closer to reality.  Don’t get me wrong, IV crush is going to keep you in the deep red on your puts without a doubt but at least you’ll recover something to use to rollforward to May - July options.\n",
      "\n",
      "SUBMISSION DISCUSSING 'SPY':\n",
      "If exponential patterns hold, we're looking at 100K deaths by 4/15. I modeled deaths as exponentially growing 8 days ago - there was going to be 100K deaths by 4/15.\n",
      "\n",
      "I ran the numbers again tonight, and 4/15 to the day is still when the magical 100K barrier is projected to occur.\n",
      "\n",
      "If you think market sentiment is at all driven by irrational milestones like that, it'll probably mark some market instability.\n",
      "\n",
      "What I'm saying is $SPY 300 C 5/15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*(\"SUBMISSION DISCUSSING '\" + df[\"stock\"][sub] + \"':\\n\" + df[\"text\"][sub] + \"\\n\" for sub in top_stock_subs_idx[:2]), sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Now, we want to find out which words are important for each *stock*, so we're going to create several ***large documents, one for each stock***. Each document includes all the tokens related to the same stock. We will also have a document including discussions that do not relate to the top 15 stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to contain a corpus of the tokens relating to each of the tickers\n",
    "stock_corpus = {}        \n",
    "for topic in df[\"stock\"].unique(): # index to exlude \"other\"\n",
    "       stock_corpus[topic] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# First append all list of tokens to the associated ticker in the dictionary. \n",
    "# Each of the dict values will be list of lists\n",
    "for i in range(len(df)):\n",
    "    #print(i)\n",
    "    stock_corpus[df[\"stock\"][i]].append(df[\"tokens\"][i])\n",
    "\n",
    "# Collapse the lists of lists\n",
    "for key in stock_corpus.keys():\n",
    "    stock_corpus[key] = list(itertools.chain.from_iterable(stock_corpus[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4. Now, we're ready to calculate the TF for each word. Find the top 5 terms within __5 stocks of your choice__. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(doc, n):\n",
    "    fd = nltk.FreqDist(doc).most_common(n)\n",
    "    tf_list = [fd[i][1]/len(doc) for i in range(n)]\n",
    "    return [(fd[i][0], round(tf_list[i]*100,3)) for i in range(n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GameStop / GME:          [('gme', 8.075), ('long', 7.248), ('i', 1.74), ('amp', 1.433), ('gt', 1.353)]\n",
      "Virgin Galactic / SPCE:  [('i', 2.474), ('spce', 2.34), ('the', 1.119), ('s', 1.081), ('amp', 0.933)]\n",
      "Microsoft / MSFT:        [('price', 2.007), ('msft', 1.682), ('amp', 1.456), ('i', 1.449), ('option', 1.385)]\n",
      "Apple / AAPL:            [('i', 2.258), ('s', 1.335), ('aapl', 1.304), ('amp', 1.276), ('earnings', 1.111)]\n",
      "Disney / DIS:            [('i', 1.732), ('amp', 1.484), ('gt', 1.484), ('earnings', 1.412), ('the', 1.258)]%\n"
     ]
    }
   ],
   "source": [
    "# We investigate the term frequencies of the top 5 terms of the following 5 stocks\n",
    "print(\"GameStop / GME:          {}\".format(term_freq(stock_corpus['GME'], 5)))\n",
    "print(\"Virgin Galactic / SPCE:  {}\".format(term_freq(stock_corpus['SPCE'], 5))) \n",
    "print(\"Microsoft / MSFT:        {}\".format(term_freq(stock_corpus['MSFT'], 5)))\n",
    "print(\"Apple / AAPL:            {}\".format(term_freq(stock_corpus['AAPL'], 5)))\n",
    "print(\"Disney / DIS:            {}\".format(term_freq(stock_corpus['DIS'], 5))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   * Describe similarities and differences between the stocks.\n",
    ">   * Why aren't the TFs not necessarily a good description of the stocks?\n",
    ">   * Next, we calculate IDF for every word. \n",
    ">   * What base logarithm did you use? Is that important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> 5. We're ready to calculate TF-IDF. Do that for the __5 stock of your choice__. \n",
    ">   * List the 10 top TF words for each stock.\n",
    ">  * List the 10 top TF-IDF words for each stock.\n",
    ">   * Are these 10 words more descriptive of the stock? If yes, what is it about IDF that makes the words more informative?\n",
    "> 6. Visualize the results in a Wordcloud and comment your results (follow the instrutions in Week 6, Exercise 4). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Creating Word Shifts_\n",
    ">    1. Pick a day of your choice in 2020. We call it $d$. It is more interesting if you pick a day where you expect something relevant to occur (e.g. Christmas, New Year, Corona starting, the market crashes...).\n",
    ">    2. Build two lists $l$ and $l_{ref}$ containing all tokens for submissions posted on r/wallstreebets on day $d$, and in the 7 days preceding day $d$, respectively. \n",
    ">    3. For each token $i$, compute the relative frequency in the two lists $l$ and $l_{ref}$. We call them $p(i,l)$ and $p(i,l_{ref})$, respectively. The relative frequency is computed as the number of times a token occurs over the total length of the document. Store the result in a dictionary.\n",
    ">    4. For each token $i$, compute the difference in relative frequency $\\delta p(i) = p(i,l) - p(i,l_{ref})$. Store the values in a dictionary. Print the top 10 tokens (those with largest relative frequency). Do you notice anything interesting?\n",
    ">    5. Now, for each token, compute the happiness $h(i) = labMT(i) - 5$, using the labMT dictionary. Here, we subtract $5$, so that positive tokens will have a positive value and negative tokens will have a negative value. Then, compute the product $\\delta \\Phi = h(i)\\cdot \\delta p(i)$. Store the results in a dictionary. \n",
    ">    6. Print the top 10 tokens, ordered by the absolute value of $|\\delta \\Phi|$. Explain in your own words the meaning of $\\delta \\Phi$. If that is unclear, have a look at [this page](https://shifterator.readthedocs.io/en/latest/cookbook/weighted_avg_shifts.html).\n",
    ">    7. Now install the [``shifterator``](https://shifterator.readthedocs.io/en/latest/installation.html) Python package. We will use it for plotting Word Shifts. \n",
    ">    8. Use the function ``shifterator.WeightedAvgShift`` to plot the WordShift, showing which words contributed the most to make your day of choice _d_ happier or more sad then days in the preceding 7 days. Comment on the figure. \n",
    ">    9. How do words that you printed in step 6 relate to those shown by the WordShift? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Communities for the Zachary Karate Club Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Zachary's karate club_: In this exercise, we will work on Zarachy's karate club graph (refer to the Introduction of Chapter 9). The dataset is available in NetworkX, by calling the function [karate_club_graph](https://networkx.org/documentation/stable/auto_examples/graph/plot_karate_club.html).\n",
    "\n",
    "> 1. Visualize the graph using [netwulf](https://netwulf.readthedocs.io/en/latest/). Set the color of each node based on the club split (the information is stored as a node attribute). My version of the visualization is below.\n",
    ">\n",
    "> 2. Write a function to compute the __modularity__ of a graph partitioning (use **equation 9.12** in the book). The function should take a networkX Graph and a partitioning as inputs and return the modularity.\n",
    "> 3. Explain in your own words the concept of _modularity_. \n",
    "> 4. Compute the modularity of the Karate club split partitioning using the function you just wrote. Note: the Karate club split partitioning is avilable as a [node attribute](https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.classes.function.get_node_attributes.html), called _\"club\"_.\n",
    "> 5. We will now perform a small randomization experiment to assess if the modularity you just computed is statitically different from $0$. To do so, we will implement a [configuration model](https://en.wikipedia.org/wiki/Configuration_model). In short, we will create a new network, such that each node has the same degree as in the original network, but different connections. Here is how the algorithm works.\n",
    ">       * __a.__ Create an identical copy of your original network. \n",
    ">       * __b.__ Consider the list of network edges. Create two lists: the list of source nodes and target nodes. (e.g. edges = [(1,2),(3,4)], sources = [1,3], targets = [2,4])\n",
    ">       * __c.__ Concatenate the list of source nodes and target nodes into a unique list (e.g. [1,3,2,4]). This is the list of _stubs_ (see the [Wikipedia page](https://en.wikipedia.org/wiki/Configuration_model) for the definition of stub).\n",
    ">       * __d.__ Shuffle the list of stubs. Build a set of edges (tuples), by connecting each element in the list of shuffled stubs with the following element. (e.g. [4,1,2,3] --> [(4,1),(2,3)])\n",
    ">       * __e.__ Remove all the original network edges from your network. Add all the new _shuffled_ edges you created in step __d.__\n",
    "> 6. Is the degree of the nodes in your original and the configuration model network the same? Why? __Note 1:__ With this algorithm you may obtain some self-loops. Note that [a self-loop should add two to the degree](https://en.wikipedia.org/wiki/Loop_(graph_theory%29#:~:text=For%20an%20undirected%20graph%2C%20the,adds%20two%20to%20the%20degree.&text=In%20other%20words%2C%20a%20vertex,not%20one%2C%20to%20the%20degree.). __Note 2:__ With this algorithm, you could also obtain repeated edges between the same two nodes. Only NetworkX [MultiGraph](https://networkx.org/documentation/stable/reference/classes/multigraph.html) allow for repeated edges, while regular [Graph](https://networkx.org/documentation/stable/reference/classes/graph.html?highlight=graph%20undirected#networkx.Graph) do not, meaning you will not be able to account for multi-edges when you have a regular Graph. (_Optional_: if you want to implement a configuration model without self-loops and multi-edges, you can try out the [double_edge_swap](https://networkx.org/documentation/stable//reference/algorithms/generated/networkx.algorithms.swap.double_edge_swap.html) algorithm)\n",
    "> 7. Create $1000$ randomized version of the Karate Club network using the algorithm you wrote in step 5. For each of them, compute the modularity of the \"club\" split and store it in a list.\n",
    "> 8. Compute the average and standard deviation of the modularity for the configuration model.\n",
    "> 9. Plot the distribution of the configuration model modularity. Plot the actual modularity of the club split as a vertical line (use [axvline](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axvline.html)). \n",
    "> 10. Comment on the figure. Is the club split a good partitioning? Why do you think I asked you to compare with the configuration model? What is the reason why we preserved the nodes degree?\n",
    "> 11.  Use [the Python Louvain-algorithm implementation](https://anaconda.org/auto/python-louvain) to find communities in this graph. Report the value of modularity found by the algorithm. Is it higher or lower than what you found above for the club split? What does this comparison reveal?\n",
    "> 12.  Compare the communities found by the Louvain algorithm with the club split partitioning by creating a matrix **_D_** with dimension (2 times _A_), where _A_ is the number of communities found by Louvain. We set entry _D_(_i_,_j_) to be the number of nodes that community _i_ has in common with group split _j_. The matrix **_D_** is what we call a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix). Use the confusion matrix to explain how well the communities you've detected correspond to the club split partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Community detection on the GME network._\n",
    "> * Consider the GME network you built in [Week 4](https://github.com/lalessan/comsocsci2022/blob/main/lectures/Week4.ipynb), part 2.\n",
    "> * Use [the Python Louvain-algorithm implementation](https://anaconda.org/auto/python-louvain) to find communities. How many communities do you find? What are their sizes? Report the value of modularity found by the algorithm. Is the modularity significantly different than 0? \n",
    "> * Visualize the network, using netwulf (see Week 4). This time assign each node a different color based on their _community_. Describe the structure you observe."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
