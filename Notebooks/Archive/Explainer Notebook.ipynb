{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c00de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data from the r/politics\n",
    "from psaw import PushshiftAPI\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb21eca",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "\n",
    "### The Reddit dataset\n",
    "\n",
    "For this project, we chose to work with data from the [r/politics](https://www.reddit.com/r/politics/) subreddit, an online forum with 8 million members \"for current and explicitly political U.S. news.\" according to the rules stated on the site. \n",
    "\n",
    "Visitors at r/politics will quickly notice that the majority of the submissions are by users posting links to news articles published on news media sites like CNN or The Huffington Post. The headlines of these linked articles are then shown on r/politics as the titles of the submissions. Other users can then comment on the linked article, which is what ultimately constitutes the actual user-generated content on the site. \n",
    "\n",
    "We focused our data extraction to only include submissions from r/politics that fulfilled the following criteria: \n",
    "* __They contained \"Trump\" or \"Biden\" in the title.__ While submissions containing other words and names than \"Trump\" and \"Biden\" (e.g. \"Republican\" and \"Democrat\") might be used to provide equally good indications of the political convictions of redditors, this textual query allowed us to limit the scope of the project while still extracting data essential to the aim of this project. \n",
    "* __They had received more than five comments.__ This requirement was to prevent us from downloading submissions with no or only a very small comments section, as we'll be using the comments to conduct the later sentiment analysis and produce a partitioning of the redditors. \n",
    "* __They had been published between 10-1-2020 and 11-3-2020.__ This period covered approximately a month before the most recent U.S. presidential election that took place on 11-3-2020. Ideally, we would have covered several months leading up to the election day, in order to detect longer term trends in the data. However, that would prove to be computationally infeasible, given the amount of data this would yield. \n",
    "\n",
    "__Submission variables__\n",
    "\n",
    "The downloaded submissions would be structured in a Pandas dataframe containing the following variables for each submission in its respective columns: \n",
    "1. __time stamp index:__ Simply stating when the submissions was made.\n",
    "2. __title:__ Being the title of the submission. Usually the header of the linked article. \n",
    "3. __id:__ A unique identifier for a particular submission. \n",
    "4. __author:__ The profile name of the author of the submissions.\n",
    "5. __num_comments:__ The number of comments received on the particular submission.\n",
    "6. __url:__ The link stated in the text of the submission.\n",
    "\n",
    "__Comments variables__\n",
    "\n",
    "As stated, we would download the associated comments section for all the downloaded submissions. Similarly to the submissions, the comments would be structured in a Pandas dataframe containing the following variables for each comment in its respective columns:\n",
    "1. \n",
    "\n",
    "['id', 'link_id', 'author', 'parent_id', 'body']\n",
    "\n",
    "__Reasons for choosing this particular data set__ <br>\n",
    "1. Easy to collect using the Pushshift API.\n",
    "2. Interesting topic that would fit the requirements of the project.\n",
    "3. Similar format as the data we've previously worked with.\n",
    "\n",
    "__Goal for end user's experience__ <br>\n",
    "Our goal is for the end user's of our website to have a blast digesting our the findinds of our analyses presented in a beautiful and thought-provoking way. \n",
    "\n",
    "Below, we show how we got the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a666fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lasse\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    }
   ],
   "source": [
    "# Note this is just a POC with a limit=100. \n",
    "api = PushshiftAPI()\n",
    "\n",
    "my_subreddit = \"politics\"\n",
    "query = \"Trump | Biden \"\n",
    "\n",
    "date1 = int(datetime.datetime(2020,10,1).timestamp())\n",
    "date2 = int(datetime.datetime(2020,11,3).timestamp())\n",
    "\n",
    "gen = api.search_submissions(num_comments= '>5',\n",
    "                             subreddit=my_subreddit, \n",
    "                             after=date1, \n",
    "                             before=date2, \n",
    "                             q=query\n",
    "                             ,limit=100\n",
    "                            )\n",
    "results = list(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7ab404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['title', 'id', 'author', 'num_comments', 'url']\n",
    "\n",
    "subs_df = pd.DataFrame(\n",
    "    {\n",
    "        column_names[0] : [submission.d_[column_names[0]] for submission in results],\n",
    "        column_names[1] : [submission.d_[column_names[1]] for submission in results],\n",
    "        column_names[2] : [submission.d_[column_names[2]] for submission in results],\n",
    "        column_names[3] : [submission.d_[column_names[3]] for submission in results],\n",
    "        column_names[4] : [submission.d_[column_names[4]] for submission in results]\n",
    "    },\n",
    "    index = [submission.d_['created_utc'] for submission in results])\n",
    "subs_df.index = pd.to_datetime(subs_df.index, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4131087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-02 22:54:58</th>\n",
       "      <td>Trump ramps up Fauci attacks on eve of electio...</td>\n",
       "      <td>jmybs3</td>\n",
       "      <td>geoxol</td>\n",
       "      <td>33</td>\n",
       "      <td>https://thehill.com/homenews/administration/52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 22:48:58</th>\n",
       "      <td>Trump Loves To Declare Victory Even if He Didn...</td>\n",
       "      <td>jmy7vu</td>\n",
       "      <td>Facerealityalready</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.motherjones.com/politics/2020/11/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 22:46:57</th>\n",
       "      <td>Trump creates 1776 Commission to promote 'patr...</td>\n",
       "      <td>jmy6j9</td>\n",
       "      <td>bluestblue</td>\n",
       "      <td>53</td>\n",
       "      <td>https://www.politico.com/news/2020/11/02/trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 22:42:26</th>\n",
       "      <td>Judge blocks Trump campaign challenge to Nevad...</td>\n",
       "      <td>jmy3hn</td>\n",
       "      <td>TrumpSharted</td>\n",
       "      <td>10</td>\n",
       "      <td>https://thehill.com/homenews/state-watch/52403...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 22:41:51</th>\n",
       "      <td>Trump boasts about newspaper endorsement that ...</td>\n",
       "      <td>jmy340</td>\n",
       "      <td>Zhana-Aul</td>\n",
       "      <td>23</td>\n",
       "      <td>https://www.independent.co.uk/news/world/ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:18:22</th>\n",
       "      <td>How Trump and Barr’s October Surprise Went Bust</td>\n",
       "      <td>jmu50j</td>\n",
       "      <td>i-am-sancho</td>\n",
       "      <td>25</td>\n",
       "      <td>https://nymag.com/intelligencer/2020/11/durham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:18:18</th>\n",
       "      <td>Who’s Giving to Trump and Biden? Top Donors by...</td>\n",
       "      <td>jmu4yn</td>\n",
       "      <td>ttkk1248</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.bloomberg.com/graphics/2020-electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:16:48</th>\n",
       "      <td>Eminem signals Biden support as campaign relea...</td>\n",
       "      <td>jmu410</td>\n",
       "      <td>wwabc</td>\n",
       "      <td>11</td>\n",
       "      <td>https://www.freep.com/story/entertainment/musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:14:18</th>\n",
       "      <td>Eminem Licenses ‘Lose Yourself’ for Biden-Harr...</td>\n",
       "      <td>jmu2e0</td>\n",
       "      <td>chanma50</td>\n",
       "      <td>661</td>\n",
       "      <td>https://variety.com/2020/music/news/eminem-lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:14:10</th>\n",
       "      <td>Donald Trump tries to stoke fears of Covid loc...</td>\n",
       "      <td>jmu2b4</td>\n",
       "      <td>AGAKILLER1129</td>\n",
       "      <td>26</td>\n",
       "      <td>https://www.theguardian.com/us-news/2020/nov/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 title  \\\n",
       "2020-11-02 22:54:58  Trump ramps up Fauci attacks on eve of electio...   \n",
       "2020-11-02 22:48:58  Trump Loves To Declare Victory Even if He Didn...   \n",
       "2020-11-02 22:46:57  Trump creates 1776 Commission to promote 'patr...   \n",
       "2020-11-02 22:42:26  Judge blocks Trump campaign challenge to Nevad...   \n",
       "2020-11-02 22:41:51  Trump boasts about newspaper endorsement that ...   \n",
       "...                                                                ...   \n",
       "2020-11-02 19:18:22    How Trump and Barr’s October Surprise Went Bust   \n",
       "2020-11-02 19:18:18  Who’s Giving to Trump and Biden? Top Donors by...   \n",
       "2020-11-02 19:16:48  Eminem signals Biden support as campaign relea...   \n",
       "2020-11-02 19:14:18  Eminem Licenses ‘Lose Yourself’ for Biden-Harr...   \n",
       "2020-11-02 19:14:10  Donald Trump tries to stoke fears of Covid loc...   \n",
       "\n",
       "                         id              author  num_comments  \\\n",
       "2020-11-02 22:54:58  jmybs3              geoxol            33   \n",
       "2020-11-02 22:48:58  jmy7vu  Facerealityalready            16   \n",
       "2020-11-02 22:46:57  jmy6j9          bluestblue            53   \n",
       "2020-11-02 22:42:26  jmy3hn        TrumpSharted            10   \n",
       "2020-11-02 22:41:51  jmy340           Zhana-Aul            23   \n",
       "...                     ...                 ...           ...   \n",
       "2020-11-02 19:18:22  jmu50j         i-am-sancho            25   \n",
       "2020-11-02 19:18:18  jmu4yn            ttkk1248             8   \n",
       "2020-11-02 19:16:48  jmu410               wwabc            11   \n",
       "2020-11-02 19:14:18  jmu2e0            chanma50           661   \n",
       "2020-11-02 19:14:10  jmu2b4       AGAKILLER1129            26   \n",
       "\n",
       "                                                                   url  \n",
       "2020-11-02 22:54:58  https://thehill.com/homenews/administration/52...  \n",
       "2020-11-02 22:48:58  https://www.motherjones.com/politics/2020/11/t...  \n",
       "2020-11-02 22:46:57  https://www.politico.com/news/2020/11/02/trump...  \n",
       "2020-11-02 22:42:26  https://thehill.com/homenews/state-watch/52403...  \n",
       "2020-11-02 22:41:51  https://www.independent.co.uk/news/world/ameri...  \n",
       "...                                                                ...  \n",
       "2020-11-02 19:18:22  https://nymag.com/intelligencer/2020/11/durham...  \n",
       "2020-11-02 19:18:18  https://www.bloomberg.com/graphics/2020-electi...  \n",
       "2020-11-02 19:16:48  https://www.freep.com/story/entertainment/musi...  \n",
       "2020-11-02 19:14:18  https://variety.com/2020/music/news/eminem-lic...  \n",
       "2020-11-02 19:14:10  https://www.theguardian.com/us-news/2020/nov/0...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e006ce",
   "metadata": {},
   "source": [
    "To ensure that each submissions is unabigously related to either one of the candidates, we simply remove all submissions containing both \"Trump\" and \"Biden\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fcaf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to contain indices of subs in df with a title containing both \"Trump\" and \"Biden\"\n",
    "TB = []\n",
    "for i in range(len(df['title'])):\n",
    "    if (re.search('Trump', subs_df['title'][i])) and (re.search('Biden', subs_df['title'][i])):\n",
    "        TB.append(i)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "subs_df = subs_df.drop(subs_df.index[TB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05866f7f",
   "metadata": {},
   "source": [
    "Now we're ready to download the associated comments sections for each of the remaining submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f879fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82 [00:00<?, ?it/s]C:\\Users\\Lasse\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      " 21%|██        | 17/82 [02:19<05:06,  4.71s/it]C:\\Users\\Lasse\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\Lasse\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "100%|██████████| 82/82 [05:26<00:00,  3.98s/it]\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "for link_id in tqdm(subs_df['id']):\n",
    "    gen = api.search_comments(subreddit=my_subreddit,\n",
    "                              link_id=link_id)\n",
    "    comment_sec = list(gen)\n",
    "    comments += comment_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2c613fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id', 'link_id', 'author', 'parent_id', 'body']\n",
    "\n",
    "coms_df = pd.DataFrame(\n",
    "    {\n",
    "        column_names[0] : [comment.d_[column_names[0]] for comment in comments],\n",
    "        column_names[1] : [comment.d_[column_names[1]] for comment in comments],\n",
    "        column_names[2] : [comment.d_[column_names[2]] for comment in comments],\n",
    "        column_names[3] : [comment.d_[column_names[3]] for comment in comments],\n",
    "        column_names[4] : [comment.d_[column_names[4]] for comment in comments]\n",
    "    },\n",
    "    columns= column_names, index = [comment.d_['created_utc'] for comment in comments])\n",
    "\n",
    "coms_df.index = pd.to_datetime(coms_df.index, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f32b8",
   "metadata": {},
   "source": [
    "Some of these comments have been removed after being posted, so we'll do some cleaning first by filtering out the comments where author = \"[deleted]\", which will do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40fa3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "coms_df = coms_df[coms_df.author != \"[deleted]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b214449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-03 01:38:03</th>\n",
       "      <td>gaykxxw</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>saint-cecelia</td>\n",
       "      <td>t1_gayg0dt</td>\n",
       "      <td>He got on eminem, too? I can't keep up. \\n\\nI'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03 00:52:13</th>\n",
       "      <td>gayg0dt</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>NorweAmeriLove</td>\n",
       "      <td>t1_gay44pe</td>\n",
       "      <td>and Eminem!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03 00:48:23</th>\n",
       "      <td>gayflfy</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>ChickenNPisza</td>\n",
       "      <td>t1_gaye3oj</td>\n",
       "      <td>They are all saving face I bet. Contradict the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03 00:34:43</th>\n",
       "      <td>gaye3oj</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>droplivefred</td>\n",
       "      <td>t1_gay339x</td>\n",
       "      <td>That his been his strategy since summer and hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03 00:23:36</th>\n",
       "      <td>gaycvmz</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>yyungpiss</td>\n",
       "      <td>t3_jmybs3</td>\n",
       "      <td>is there some sort of weird strategy to this o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:24:57</th>\n",
       "      <td>gaxd8l3</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>Das_Man</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>Whatever happens tomorrow, Biden's ad team des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:23:10</th>\n",
       "      <td>gaxd0ot</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>F6Pilot</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>Quality move, Slim Shady!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:18:47</th>\n",
       "      <td>gaxcgxe</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>BroadAsparagus</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>Not Afraid would also make a good campaign ad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:18:03</th>\n",
       "      <td>gaxcdph</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>Shwetty_Morrow</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>Yes.\\n\\nSo much yes.\\n\\nEpic win, much?\\n\\nAlt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02 19:14:18</th>\n",
       "      <td>gaxbww1</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>t3_jmu2e0</td>\n",
       "      <td>\\nRegister to vote or check your registration ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10703 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id    link_id          author   parent_id  \\\n",
       "2020-11-03 01:38:03  gaykxxw  t3_jmybs3   saint-cecelia  t1_gayg0dt   \n",
       "2020-11-03 00:52:13  gayg0dt  t3_jmybs3  NorweAmeriLove  t1_gay44pe   \n",
       "2020-11-03 00:48:23  gayflfy  t3_jmybs3   ChickenNPisza  t1_gaye3oj   \n",
       "2020-11-03 00:34:43  gaye3oj  t3_jmybs3    droplivefred  t1_gay339x   \n",
       "2020-11-03 00:23:36  gaycvmz  t3_jmybs3       yyungpiss   t3_jmybs3   \n",
       "...                      ...        ...             ...         ...   \n",
       "2020-11-02 19:24:57  gaxd8l3  t3_jmu2e0         Das_Man   t3_jmu2e0   \n",
       "2020-11-02 19:23:10  gaxd0ot  t3_jmu2e0         F6Pilot   t3_jmu2e0   \n",
       "2020-11-02 19:18:47  gaxcgxe  t3_jmu2e0  BroadAsparagus   t3_jmu2e0   \n",
       "2020-11-02 19:18:03  gaxcdph  t3_jmu2e0  Shwetty_Morrow   t3_jmu2e0   \n",
       "2020-11-02 19:14:18  gaxbww1  t3_jmu2e0   AutoModerator   t3_jmu2e0   \n",
       "\n",
       "                                                                  body  \n",
       "2020-11-03 01:38:03  He got on eminem, too? I can't keep up. \\n\\nI'...  \n",
       "2020-11-03 00:52:13                                        and Eminem!  \n",
       "2020-11-03 00:48:23  They are all saving face I bet. Contradict the...  \n",
       "2020-11-03 00:34:43  That his been his strategy since summer and hi...  \n",
       "2020-11-03 00:23:36  is there some sort of weird strategy to this o...  \n",
       "...                                                                ...  \n",
       "2020-11-02 19:24:57  Whatever happens tomorrow, Biden's ad team des...  \n",
       "2020-11-02 19:23:10                          Quality move, Slim Shady!  \n",
       "2020-11-02 19:18:47     Not Afraid would also make a good campaign ad.  \n",
       "2020-11-02 19:18:03  Yes.\\n\\nSo much yes.\\n\\nEpic win, much?\\n\\nAlt...  \n",
       "2020-11-02 19:14:18  \\nRegister to vote or check your registration ...  \n",
       "\n",
       "[10703 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72ae98cf",
   "metadata": {},
   "source": [
    "### Why this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d000ae",
   "metadata": {},
   "source": [
    "# 2 Basic stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306c3ca",
   "metadata": {},
   "source": [
    "# 3 Tools, theory and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe883a",
   "metadata": {},
   "source": [
    "# 4 Discussion\n",
    "\n",
    "### What is still missing?\n",
    "\n",
    "### What could be improved?\n",
    "\n",
    "It would undoubtedly have been interesting to investigate the political content on r/politics over a longer time period, e.g. six months preceeding the election day, which would allow for the detection of longer term trends in redditor activity and sentiment. For such a scope to be feasible, bigger computational muscles than what the group members had at their disposal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
